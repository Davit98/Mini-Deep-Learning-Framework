{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x109f18cf8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our main class from which a lot of other classes will inherit (e.g. fully connected layer)\n",
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        self.grad_input = None\n",
    "        self.train = True\n",
    "    \n",
    "    def forward(self, inpt):\n",
    "        return self.update_output(inpt)\n",
    "\n",
    "    def backward(self, inpt, grad_output):\n",
    "        self.update_grad_input(inpt, grad_output)\n",
    "        self.acc_grad_params(inpt, grad_output)\n",
    "        return self.grad_input\n",
    "    \n",
    "    def update_output(self, inpt):        \n",
    "        pass\n",
    "\n",
    "    # gradient with respect to input\n",
    "    def update_grad_input(self, inpt, grad_output):       \n",
    "        pass   \n",
    "    \n",
    "    # gradient with respect to parameters\n",
    "    def acc_grad_params(self, inpt, grad_output):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad_params(self): \n",
    "        pass\n",
    "        \n",
    "    def get_params(self):\n",
    "        return []\n",
    "        \n",
    "    def get_grad_params(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a container class like keras's Sequential model\n",
    "class Sequential(Module):    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "        self.y = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def update_output(self, inpt): \n",
    "        self.y = [] \n",
    "\n",
    "        y_p = self.modules[0].forward(inpt)\n",
    "        self.y.append(y_p)\n",
    "        for i in range(1,len(self.modules)):\n",
    "            y_n = self.modules[i].forward(y_p)\n",
    "            self.y.append(y_n)\n",
    "            y_p = y_n\n",
    "        \n",
    "        self.output = y_n\n",
    "            \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, inpt, grad_output): \n",
    "        n = len(self.modules)\n",
    "        g_n = self.modules[n-1].backward(self.y[n-2],grad_output)\n",
    "        for i in range(n-2,0,-1):\n",
    "            g_p = self.modules[i].backward(self.y[i-1],g_n)\n",
    "            g_n = g_p\n",
    "            \n",
    "        self.grad_input = self.modules[0].backward(inpt,g_p)\n",
    "        return self.grad_input\n",
    "\n",
    "    def zero_grad_params(self): \n",
    "        for module in self.modules:\n",
    "            module.zero_grad_params()\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [x.get_params() for x in self.modules]\n",
    "    \n",
    "    def get_grad_params(self):\n",
    "        return [x.get_grad_params() for x in self.modules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: batch_size x n_features1\n",
    "# output: batch_size x n_features2\n",
    "\n",
    "class DenseLayer(Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(DenseLayer, self).__init__()\n",
    "       \n",
    "        #initializing weights \n",
    "        stdv = 1./math.sqrt(n_in)\n",
    "        self.W = torch.FloatTensor(n_out,n_in).uniform_(-stdv,stdv)\n",
    "        self.b = torch.FloatTensor(n_out).uniform_(-stdv,stdv)\n",
    "        \n",
    "        self.gradW = torch.zeros_like(self.W)\n",
    "        self.gradb = torch.zeros_like(self.b)\n",
    "    \n",
    "    def update_output(self, inpt):\n",
    "        self.output = torch.mm(inpt,self.W.T) + self.b  \n",
    "        return self.output\n",
    "\n",
    "    def update_grad_input(self, inpt, grad_output):\n",
    "        self.grad_input = torch.mm(grad_output,self.W)\n",
    "        return self.grad_input\n",
    "    \n",
    "    def acc_grad_params(self, inpt, grad_output):\n",
    "        self.gradW = torch.mm(grad_output,input)\n",
    "        self.gradb = grad_output.sum(axis=0) \n",
    "    \n",
    "    def zero_grad_params(self):\n",
    "        self.gradW = torch.zeros_like(self.gradW)\n",
    "        self.gradb = torch.zeros_like(self.gradb)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def get_grad_params(self):\n",
    "        return [self.gradW, self.gradb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_layer = DenseLayer(2, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_layer.get_grad_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.4079,  0.2833],\n",
       "         [-0.6016, -0.5495],\n",
       "         [-0.4683, -0.6694],\n",
       "         [ 0.2851,  0.0040],\n",
       "         [ 0.4665,  0.3928],\n",
       "         [ 0.6579,  0.1451],\n",
       "         [-0.1126,  0.5906],\n",
       "         [ 0.0165,  0.0917],\n",
       "         [ 0.2582, -0.3816],\n",
       "         [-0.4919,  0.2812],\n",
       "         [-0.2568,  0.3943],\n",
       "         [-0.5193, -0.2884],\n",
       "         [-0.6625, -0.0255],\n",
       "         [-0.5337, -0.2044],\n",
       "         [-0.6906,  0.0825],\n",
       "         [-0.7061, -0.5512],\n",
       "         [-0.0746, -0.1444],\n",
       "         [ 0.4210,  0.4040],\n",
       "         [ 0.0886, -0.0594],\n",
       "         [-0.3850,  0.1256],\n",
       "         [ 0.5332, -0.4738],\n",
       "         [-0.1632, -0.5522],\n",
       "         [ 0.6804,  0.0813],\n",
       "         [ 0.4823,  0.1149],\n",
       "         [ 0.1545, -0.4765],\n",
       "         [ 0.6300,  0.1906],\n",
       "         [-0.4823, -0.3331],\n",
       "         [ 0.1652, -0.2538],\n",
       "         [-0.4403,  0.5474],\n",
       "         [ 0.5304, -0.6658]]),\n",
       " tensor([ 0.2682,  0.2498,  0.5760,  0.3510, -0.5641,  0.2335,  0.2658, -0.1946,\n",
       "          0.1685, -0.5998,  0.5618, -0.6468, -0.2686, -0.6889,  0.6856,  0.4943,\n",
       "         -0.2512,  0.6103, -0.0145,  0.4648,  0.5271,  0.1601,  0.5480, -0.5403,\n",
       "          0.2115,  0.1069,  0.2362, -0.2293,  0.2650, -0.1746])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_layer.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "         super(Tanh, self).__init__()\n",
    "    \n",
    "    def update_output(self, inpt):\n",
    "        self.output = torch.tanh(inpt)\n",
    "        return self.output\n",
    "    \n",
    "    def update_grad_input(self, inpt, grad_output):\n",
    "        self.grad_input = (1 - self.output**2)*grad_output\n",
    "        return self.grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTING\n",
    "\n",
    "# class Animal(object):\n",
    "#     def __init__(self,domestic,colour):\n",
    "#         self.name = None\n",
    "#         self.domestic = domestic\n",
    "#         self.colour = colour\n",
    "    \n",
    "#     def sound(self):\n",
    "#         pass\n",
    "    \n",
    "#     def reality(self):\n",
    "#         print('Humans are more clever than any other animal on the planet')\n",
    "        \n",
    "#     def __repr__(self):\n",
    "#         return \"Module\"\n",
    "        \n",
    "\n",
    "# class Haski(Animal):\n",
    "#     def __init__(self, domestic, colour, name):\n",
    "#         super().__init__(domestic,colour)\n",
    "#         self.name = name\n",
    "        \n",
    "#     def sound(self):\n",
    "#         print('Haf-Haf')\n",
    "        \n",
    "#     def __repr__(self):\n",
    "#         return \"Dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dog = Haski(True,'White','Leo')\n",
    "# my_dog.reality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
